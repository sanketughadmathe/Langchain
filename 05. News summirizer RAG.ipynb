{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import StrOutputParser\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class HackerNewsFetcher:\n",
    "    \"\"\"A class to fetch articles from Hacker News.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://hacker-news.firebaseio.com/v0\"\n",
    "\t\t\n",
    "\n",
    "    def _fetch_article(self, article_id: int) -> dict:\n",
    "        \"\"\"Fetch an article from the Hacker News API.\n",
    "\n",
    "        Args:\n",
    "            article_id (int): The ID of the article to fetch.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the article's data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            post = requests.get(f\"{self.base_url}/item/{article_id}.json\").json()\n",
    "            return post\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to fetch article {article_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _fetch_content(self, url: str) -> str:\n",
    "        \"\"\"Fetch the content of a web page.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL of the web page.\n",
    "\n",
    "        Returns:\n",
    "            str: The content of the web page as text.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            return soup.get_text()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to fetch content from {url}: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "\n",
    "    def fetch_articles(self, top_k: int) -> List[dict]:\n",
    "        \"\"\"Fetch the top K articles from Hacker News.\n",
    "\n",
    "        Args:\n",
    "            top_k (int): The number of articles to fetch.\n",
    "\n",
    "        Returns:\n",
    "            List[dict]: A list of dictionaries containing the article's data.\n",
    "        \"\"\"\n",
    "        response = requests.get(f\"{self.base_url}/topstories.json\")\n",
    "        trending_list = response.json()\n",
    "\n",
    "        articles = []\n",
    "        for id in trending_list[:top_k]:\n",
    "            post = self._fetch_article(id)\n",
    "            if post and \"url\" in post:\n",
    "                content = self._fetch_content(post[\"url\"])\n",
    "                article_data = {\n",
    "                    \"content\": content,\n",
    "                    \"title\": post[\"title\"],\n",
    "                    \"url\": post[\"url\"],\n",
    "                }\n",
    "                articles.append(article_data)\n",
    "\n",
    "        return articles\n",
    "\n",
    "\n",
    "class ArticleSummarizer:\n",
    "    \"\"\"A class to summarize articles fetched from Hacker News.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.prompt_template = \"\"\"\n",
    "            Summarize the following article from HackerNews. Provide a brief summary in the following format:\n",
    "            URL:\n",
    "            Summary:\n",
    "            \n",
    "\t\t\tThe full post can be found at.\n",
    "            \n",
    "            Article:\n",
    "            Title: {title}\n",
    "            Content: {content}\n",
    "            URL: {url}\n",
    "\n",
    "            Summary:\n",
    "            \"\"\"\n",
    "\n",
    "    def summarize_articles(self, articles: List[dict]) -> str:\n",
    "        \"\"\"Summarize a list of articles fetched from Hacker News.\n",
    "\n",
    "        Args:\n",
    "            articles (List[dict]): A list of dictionaries containing the article's data.\n",
    "\n",
    "        Returns:\n",
    "            str: A string summarizing each article.\n",
    "        \"\"\"\n",
    "        llm = ChatOllama(model=\"llama3.1:8b\", temperature=0)\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"title\", \"content\", \"url\"], template=self.prompt_template\n",
    "        )\n",
    "\n",
    "        chain = (prompt_template | llm | StrOutputParser()).invoke\n",
    "\n",
    "        summaries = []\n",
    "        for article in articles:\n",
    "            summary = chain(\n",
    "                {\n",
    "                    \"title\": article[\"title\"],\n",
    "                    \"content\": article[\"content\"],\n",
    "                    \"url\": article[\"url\"],\n",
    "                }\n",
    "            )\n",
    "            summaries.append(summary)\n",
    "\n",
    "        return \"\\n-------------\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "fetcher = HackerNewsFetcher()\n",
    "articles = fetcher.fetch_articles(top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Create Calendar Entries with Anthropic Claude 3.5Greg’s RamblingsFollowGreg’s RamblingsFollowStupid but useful AI tricks: Creating calendar entries from an image using Anthropic Claude 3.5Greg Wilson·Aug 25, 2024·4 min readA few days ago, my jazz piano teacher sent me the new fall/winter schedule for my private jazz piano lessons -- 13 different dates -- as a JPG (mine are outlined in green marker):\\n\\nI was too lazy to go make 13 entries in Google Calendar, so I decided to see if Claude could help me out:\\nI first uploaded the jpg to Claude 3.5 Sonnet)...\\n\\nMy prompt: List the dates that are outlined in green\\nThe dates outlined in green in the image are:\\nFriday, 13-Sep-24 Friday, 27-Sep-24 Friday, 11-Oct-24 Friday, 8-Nov-24 Friday, 6-Dec-24 Friday, 20-Dec-24 Friday, 17-Jan-25 Friday, 31-Jan-25 Friday, 28-Feb-25 Friday, 14-Mar-25 Friday, 11-Apr-25 Friday, 25-Apr-25 Friday, 23-May-25\\nThese dates correspond to the Friday column for students who have their first every-other-week lesson starting the week of Tuesday Sept 10, 2024.\\n\\nCool - that was easy and accurate. Now I need to get it into my calendar, so I asked it to create an ics file...\\n\\nMy prompt: Create a calendar ics file that creates an appointment on each of those dates at 2pm pacific time. The appointment title will be “Jazz Piano Lesson”\\nCertainly. I\\'ll create an ICS (iCalendar) file with appointments for \"Jazz Piano Lesson\" on each of the dates you specified, set for 2:00 PM Pacific Time.\\n\\nBEGIN:VCALENDAR\\nVERSION:2.0\\nPRODID:-//Claude//Jazz Piano Lessons//EN\\nCALSCALE:GREGORIAN\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20240913T210000Z\\nDTEND:20240913T220000Z\\nDTSTAMP:20240824T000000Z\\nUID:20240913jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20240927T210000Z\\nDTEND:20240927T220000Z\\nDTSTAMP:20240824T000000Z\\nUID:20240927jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20241011T210000Z\\nDTEND:20241011T220000Z\\nDTSTAMP:20240824T000000Z\\nUID:20241011jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20241108T220000Z\\nDTEND:20241108T230000Z\\nDTSTAMP:20240824T000000Z\\nUID:20241108jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20241206T220000Z\\nDTEND:20241206T230000Z\\nDTSTAMP:20240824T000000Z\\nUID:20241206jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20241220T220000Z\\nDTEND:20241220T230000Z\\nDTSTAMP:20240824T000000Z\\nUID:20241220jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20250117T220000Z\\nDTEND:20250117T230000Z\\nDTSTAMP:20240824T000000Z\\nUID:20250117jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20250131T220000Z\\nDTEND:20250131T230000Z\\nDTSTAMP:20240824T000000Z\\nUID:20250131jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20250228T220000Z\\nDTEND:20250228T230000Z\\nDTSTAMP:20240824T000000Z\\nUID:20250228jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20250314T210000Z\\nDTEND:20250314T220000Z\\nDTSTAMP:20240824T000000Z\\nUID:20250314jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20250411T210000Z\\nDTEND:20250411T220000Z\\nDTSTAMP:20240824T000000Z\\nUID:20250411jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20250425T210000Z\\nDTEND:20250425T220000Z\\nDTSTAMP:20240824T000000Z\\nUID:20250425jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20250523T210000Z\\nDTEND:20250523T220000Z\\nDTSTAMP:20240824T000000Z\\nUID:20250523jazzpiano@claude.ai\\nEND:VEVENT\\nEND:VCALENDAR\\n\\n\\nI\\'ve created an ICS file with the following details:\\n\\nEach event is titled \"Jazz Piano Lesson\"\\n\\nThe events are scheduled for 2:00 PM Pacific Time on each of the specified dates\\n\\nEach event is set to last for one hour\\n\\nThe time zone is properly adjusted to UTC (Z) in the file\\n\\nUnique identifiers (UIDs) are provided for each event\\n\\n\\nYou can save this content as a .ics file and import it into most calendar applications. Here are a few notes about the file:\\n\\nThe times are in UTC (Coordinated Universal Time). 2:00 PM Pacific Time translates to 9:00 PM UTC (21:00) for dates not in Daylight Saving Time, and 10:00 PM UTC (22:00) for dates within Daylight Saving Time. The file accounts for these changes.\\n\\nThe DTSTAMP for all events is set to the date of this conversation (August 24, 2024) as a placeholder. Your calendar application will likely update this when you import the events.\\n\\nThe UID for each event is unique, based on the date of the lesson and a simple string.\\n\\n\\nTo use this file:\\n\\nCopy the content of the artifact\\n\\nPaste it into a new text file\\n\\nSave the file with a .ics extension (e.g., \"jazz_piano_lessons.ics\")\\n\\nImport this file into your preferred calendar application\\n\\n\\nMost calendar applications should be able to read this file and create the appropriate recurring events. Let me know if you need any modifications or have any questions!\\n\\nI went to calendar.google.com, imported the ics, and my appointments were imported perfectly!(To import ics files into Google calendar, go to the calendar settings, then to import/export)\\nI\\'m pleasantly surprised at how easy this was!\\nI tried the exact same thing in ChatGPT -- it identified the dates in the images, but it was unable to create an .ics file. However, it did give me some Python code to create the ics file for me.\\nicalendarclaude.aicalendarimage processing\\xa0Share this',\n",
       "  'title': 'Anthropic Claude 3.5 can create icalendar files, so I did this',\n",
       "  'url': 'https://gregsramblings.com/stupid-but-useful-ai-tricks-creating-calendar-entries-from-an-image-using-anthropic-claude-35'},\n",
       " {'content': \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nShared/Defenders think in lists. Attackers think in graphs. As long as this is true, attackers win.md at master · JohnLaTwC/Shared · GitHub\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Sign in\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n        Product\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nActions\\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\nPackages\\n        Host and manage packages\\n      \\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\nCodespaces\\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\nGitHub Copilot\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\nCode review\\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\nIssues\\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\nDiscussions\\n        Collaborate outside of code\\n      \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      All features\\n\\n    \\n\\n\\n\\n      Documentation\\n\\n    \\n\\n\\n\\n\\n\\n      GitHub Skills\\n\\n    \\n\\n\\n\\n\\n\\n      Blog\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Solutions\\n        \\n\\n\\n\\n\\n\\nBy size\\n\\n\\n\\n      Enterprise\\n\\n    \\n\\n\\n\\n      Teams\\n\\n    \\n\\n\\n\\n      Startups\\n\\n    \\n\\n\\n\\n\\nBy industry\\n\\n\\n\\n      Healthcare\\n\\n    \\n\\n\\n\\n      Financial services\\n\\n    \\n\\n\\n\\n      Manufacturing\\n\\n    \\n\\n\\n\\n\\nBy use case\\n\\n\\n\\n      CI/CD & Automation\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      DevSecOps\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n        Resources\\n        \\n\\n\\n\\n\\n\\nTopics\\n\\n\\n\\n      AI\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      Security\\n\\n    \\n\\n\\n\\n      Software Development\\n\\n    \\n\\n\\n\\n      View all\\n\\n    \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      Learning Pathways\\n\\n    \\n\\n\\n\\n\\n\\n      White papers, Ebooks, Webinars\\n\\n    \\n\\n\\n\\n\\n\\n      Customer Stories\\n\\n    \\n\\n\\n\\n      Partners\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Open Source\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub Sponsors\\n        Fund open source developers\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nThe ReadME Project\\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n        Enterprise\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnterprise platform\\n        AI-powered developer platform\\n      \\n\\n\\n\\n\\nAvailable add-ons\\n\\n\\n\\n\\n\\n\\n\\nAdvanced Security\\n        Enterprise-grade security features\\n      \\n\\n\\n\\n\\n\\n\\n\\nGitHub Copilot\\n        Enterprise-grade AI features\\n      \\n\\n\\n\\n\\n\\n\\n\\nPremium Support\\n        Enterprise-grade 24/7 support\\n      \\n\\n\\n\\n\\n\\n\\nPricing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClear\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n              Search syntax tips\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nQuery\\n\\n\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n\\n\\n\\n\\n     Cancel\\n\\n    Create saved search\\n\\n\\n\\n\\n\\n\\n\\n\\n                Sign in\\n              \\n\\n\\n                Sign up\\n              \\nReseting focus\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        JohnLaTwC\\n \\n/\\n\\nShared\\n\\nPublic\\n\\n\\n\\n\\n\\n \\n\\nNotifications\\n You must be signed in to change notification settings\\n\\n\\n \\n\\nFork\\n    58\\n\\n\\n\\n\\n \\n\\n\\n          Star\\n 421\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCode\\n\\n\\n\\n\\n\\n\\n\\nIssues\\n0\\n\\n\\n\\n\\n\\n\\nPull requests\\n1\\n\\n\\n\\n\\n\\n\\nActions\\n\\n\\n\\n\\n\\n\\n\\nProjects\\n0\\n\\n\\n\\n\\n\\n\\nSecurity\\n\\n\\n\\n\\n\\n\\n\\nInsights\\n\\n\\n\\n \\n\\n \\n\\n\\nAdditional navigation options\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Code\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Issues\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Pull requests\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Actions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Projects\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Security\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Insights\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n   Files\\xa0masterBreadcrumbsShared/Defenders think in lists. Attackers think in graphs. As long as this is true, attackers win.md Blame  Blame        Latest commit\\xa0HistoryHistory49 lines (42 loc) · 6.53 KB\\xa0masterBreadcrumbsShared/Defenders think in lists. Attackers think in graphs. As long as this is true, attackers win.mdTopFile metadata and controlsPreviewCodeBlame49 lines (42 loc) · 6.53 KBRawDefenders think in lists. Attackers think in graphs. As long as this is true, attackers win.\\nDate: April 26, 2015\\nAuthor: @JohnLaTwC\\nDefender Mindset\\nA lot of network defense goes wrong before any contact with an adversary, starting with how defenders conceive of the battlefield. Most defenders focus on protecting their assets, prioritizing them, and sorting them by workload and business function. Defenders are awash in lists of assets—in system management services, in asset inventory databases, in BCDR spreadsheets. There's one problem with all of this. Defenders don't have a list of assets—they have a graph. Assets are connected to each other by security relationships. Attackers breach a network by landing somewhere in the graph using a technique such as spearphishing and they hack, finding vulnerable systems by navigating the graph. Who creates this graph? You do.\\nWhat Is the Graph?\\nThe graph in your network is the set of security dependencies that create equivalence classes among your assets. The design of your network, the management of your network, the software and services used on your network, and the behavior of users on your network all influence this graph. Take a domain controller for example. Bob admins the DC from a workstation. If that workstation is not protected as much as the domain controller, the DC can be compromised. Any other account that is an admin on Bob's workstation can compromise Bob and the DC. Every one of those admins logs on to one or more other machines in the natural course of business. If attackers compromise any of them, they have a path to compromise the DC.\\nSix Degrees of Mallory\\nAttackers can lay in wait on a compromised machine, using a password dumper such as mimikatz until a high value account logs on to the machine. Let's examine an example graph.\\n\\nThe cluster on the left is single Terminal Server used by hundreds of users. If attackers compromise this machine, they can dump the credentials of many users over time.\\n\\nHow can attackers move laterally to get to the High Value Asset?\\n\\nBy searching the graph, attackers discover multiple paths to the High Value Asset. Compromising the terminal server can allow attackers to also compromise User46 and User128. Those users are admins on Machine2821 and Machine115 respectively. Compromising those workstations allows attackers to compromise User1 and User34, both of which are admins on the High Value Asset. For the High Value Asset to be protected, all the dependent elements must be as protected as thoroughly as the HVA—forming an equivalence class.\\nSecurity Dependencies\\nIn a Windows network, when users perform certain kinds of logons (Interactive, Terminal Server, and others), those users' credentials (and single-sign-on equivalents such as a Kerberos TGT or NTLM hash) are exposed to theft if the underlying host is compromised. Beyond this, there are many kinds of relationships that create security dependencies:\\n\\nLocal admin accounts with a common password. Compromise one system, dump the local admin password, and use that password on other hosts with the same password.\\nFile servers housing logon scripts that run for many users and software update servers.\\nPrint servers that deliver print drivers to client machines when used.\\nCertificate authorities that issue certificates valid for smart card logons.\\nDatabase admins that can run code under the context of a database server running as a privileged user.\\n\\nAnd so on. There are indirect relations as well. A machine that has a vulnerability can be compromised, suddenly allowing attackers to create new edges in the graph. Or users may have an account in two untrusted domains with the same password, creating a hidden edge between domains.\\nManage your Graph\\nWhat can you do as a defender? The first step is to visualize your network by turning your lists into graphs. Next, implement controls to prune the graph:\\n\\nExamine unwanted edges that create huge connectivity bursts. Implement infrastructure partitioning and credential silos to reduce them.\\nReduce the number of admins. Use Just-In-Time / Just Enough techniques for privilege minimization.\\nUse two factor authentication to mitigate certain edge traversals.\\nApply a solid credential rotation approach in case a user account is compromised.\\nRethink forest trust relationships.\\n\\nLearn to Spot List Thinking\\nDefenders need to ensure that attackers don't have a leg up on them when visualizing the battlefield. In this contest, defenders can have the upper hand. They can have full information about their own network, whereas attackers need to study the network piece by piece. Defenders should take a lesson from how attackers come to understand the graph. Attackers study the infrastructure as it is—not as an inaccurate mental model, viewed from an incomplete asset inventory system, or a dated network diagram. Manage from reality because that's the prepared Defenders Mindset.\\nFurther Reading\\nThere are a number of papers about attack graphs. Here are a few:\\n\\nHeat-ray: Combating Identity Snowball Attacks Using Machine Learning, Combinatorial Optimization and Attack Graph by J. Dunagan, D. Simon, and A. Zheng, http://alicezheng.org/papers/sosp2009-heatray-10pt.pdf\\nTwo Formal Analyses of Attack Graphs by S. Jha, O. Sheyner and J. Wing, http://www.cs.cmu.edu/~scenariograph/jha-wing.pdf\\nUsing Model Checking to Analyze Network Vulnerabilities by P. Ammann and R. Ritchey, http://cyberunited.com/wp-content/uploads/2013/03/Using-Model-Checking-to-Analyze-Network-Vulnerabilities.pdf\\nA Graph-Based System for Network-Vulnerability Analysis by C. Phillips and L. Swiler, http://web2.utc.edu/~djy471/CPSC4660/graph-vulnerability.pdf\\nAutomated Generation and Analysis of Attack Graphs by J. Haines, S. Jha, R. Lippman, O. Sheyner, J. Wing, https://www.cs.cmu.edu/~scenariograph/sheyner-wing02.pdf\\n\\n[And thanks to @4Dgifts for mentioning the two below]\\n\\nModern Intrusion Practices by Gerardo Richarte, https://www.blackhat.com/presentations/bh-usa-03/bh-us-03-richarte.pdf\\nAttack Planning in the Real World by Jorge Lucangeli Obes, Gerardo Richarte, Carlos Sarraute, http://arxiv.org/pdf/1306.4044.pdf\\n\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nFooter\\n\\n\\n\\n\\n\\n\\n\\n\\n        © 2024 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n\\n\\n      Manage cookies\\n    \\n\\n\\n\\n\\n\\n      Do not share my personal information\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    You can’t perform that action at this time.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  'title': 'Defenders think in lists, attackers think in graphs (2015)',\n",
       "  'url': 'https://github.com/JohnLaTwC/Shared/blob/master/Defenders%20think%20in%20lists.%20Attackers%20think%20in%20graphs.%20As%20long%20as%20this%20is%20true%2C%20attackers%20win.md'},\n",
       " {'content': ' Postgres as a search engine\\n          any blockers?         \\nSubscribe\\n             Last Monday  \\n•\\n 15 minute read\\n  Postgres as a search engine Build a retrieval system with semantic, full-text, and fuzzy search in Postgres to be used as a backbone in RAG pipelines.  Search is hard. It’s a critical part of many apps, but getting it right isn’t easy. This is especially true for RAG-pipelines where the quality of retrieval can make or break the entire process.\\nWhile semantic search is trendy, good old lexical search is still the backbone. Semantic techniques can improve results, but they work best when added to a solid text-based search foundation.\\nIn this post, we’ll explore how to use Postgres to create a robust search engine. We’ll combine three techniques:\\n\\nFull-text search with tsvector\\nSemantic search with pgvector\\nFuzzy matching with pg_trgm\\nBonus: BM25\\n\\nThis approach might not be the absolute best for every situation, but it’s a great alternative to setting up a separate search service. It’s a solid starting point that you can implement and scale within your existing Postgres database.\\nI won’t go into why you should Just Use Postgres™️ for everything, but if you want to read about it, here are a couple of good resources:\\n\\nPostgres is all you need, even for vectors\\nJust Use Postgres for Everything\\nPostgreSQL is Enough\\n\\nTable with four legs\\nThis is the table we’ll work with as an example.\\ncreate table documents (\\n    id bigint primary key generated always as identity,\\n    title text,\\n    fts_title tsvector generated always as (to_tsvector(\\'english\\', title)) stored,\\n    embedding vector(1536)\\n);\\n\\nWe’ll evolve this table down the road.\\nSince Supabase has a great article on implementing hybrid search, we’ll use that as a starting point. Hybrid search\\n\\nFollow the guide to implement FTS with GIN-indexes and semantic search with pgvector (also known as bi-encoder dense retrieval).\\nFrom personal experience, I’d opt for 1536 dimensional embeddings since it’s gotten me way better results. Source: trust me.\\nI’ve replaced the Supabase function with just CTEs and a query, as well as prefixed the params with $. Here’s what it’ll look like:\\n\\nwith full_text as (\\n    select\\n        id,\\n        -- Note: ts_rank_cd is not indexable but will only rank matches of the where clause\\n        -- which shouldn\\'t be too big\\n        row_number() over(order by ts_rank_cd(fts_title, websearch_to_tsquery($query_text)) desc) as rank_ix\\n    from\\n        documents\\n    where\\n        fts_title @@ websearch_to_tsquery($query_text)\\n    order by rank_ix\\n    limit least($match_count, 30)\\n),\\nsemantic as (\\n    select\\n        id,\\n        row_number() over (order by embedding <#> $query_embedding) as rank_ix\\n    from\\n        documents\\n    order by rank_ix\\n    limit least($match_count, 30)\\n)\\nselect\\n    documents.*\\nfrom\\n    full_text\\n    full outer join semantic\\n        on full_text.id = semantic.id\\n    join documents\\n        on coalesce(full_text.id, semantic.id) = documents.id\\norder by\\n    coalesce(1.0 / ($rrf_k + full_text.rank_ix), 0.0) * $full_text_weight +\\n    coalesce(1.0 / ($rrf_k + semantic.rank_ix), 0.0) * $semantic_weight\\n    desc\\nlimit\\n    least($match_count, 30);\\n\\nNote: We’re using coalesce in several places for important reasons:\\n\\n\\nIn the join clause:\\njoin documents\\n    on coalesce(full_text.id, semantic.id) = documents.id\\n\\nThis ensures that we include results from both full-text and semantic searches, even if a document appears in only one of these result sets. If a document is found by full-text search but not by semantic search (or vice versa), we still want to include it.\\n\\n\\nIn the order by clause:\\ncoalesce(1.0 / ($rrf_k + full_text.rank_ix), 0.0) * $full_text_weight +\\ncoalesce(1.0 / ($rrf_k + semantic.rank_ix), 0.0) * $semantic_weight\\n\\nThis handles cases where a document might be present in one search result but not the other. If a document isn’t in the full-text results, its full_text.rank_ix will be NULL, so we use coalesce to treat it as 0.0 in the ranking calculation. The same applies for semantic search results.\\n\\n\\nHere we’re using Reciprocal Ranked Fusion (RRF) to merge the results.\\nList 3List 2List 1CABBCAABCRRFFinal RankingABC\\n\\nThis method ensures that items ranked high in multiple lists are given a high rank in the final list. It also ensures that items ranked high in only a few lists but low in others are not given a high rank in the final list. Placing the rank in the denominator when calculating score helps penalize the low ranking records.\\n\\nIt’s also worth noting:\\n\\n$rrf_k: To prevent extremely high scores for items ranked first (since we’re dividing by the rank), a k constant is often added to the denominator to smooth the score.\\n$ _weight: We can assign a weight to each method. This is very useful when you’re tuning the results.\\n\\nImplementing fuzzy search\\nWhile this gets us a long way, an immediate issue will be typos in named entities. While semantic search eliminates some of these issues by capturing similarity, it struggles to do so for names, acronyms, and other text that’s not semantically similar. To mitigate this, we’ll introduce the pg_trgm extension to allow for fuzzy searching.\\ncreate extension if not exists pg_trgm;\\n\\nIt operates with Trigrams. Here’s how it works:\\n\\nTrigrams are useful for fuzzy search because they break down words into three-character sequences. This allows for matching similar words even if they contain typos or slight variations. For example, “hello” and “helo” share many trigrams, making them easier to match in a fuzzy search.\\nYou have to create a new index for the desired column like this:\\ncreate index idx_documents_title_trgm on documents using gin (title gin_trgm_ops);\\n\\nAfter that, you need to add it to the full search query. The extension exposes the % operator to filter out text where similarity is larger than pg_trgm.similarity_threshold (default is 0.3). There are also several other operators that are useful. Everything is well documented here: pg_trgm — support for similarity of text using trigram matching\\nHere’s the new query with fuzzy searching implemented:\\nwith fuzzy as ( \\n    select id,\\n           similarity(title, $query_text) as sim_score,\\n           row_number() over (order by similarity(title, $query_text) desc) as rank_ix\\n    from documents\\n    where title % $query_text\\n    order by rank_ix\\n    limit least($match_count, 30)\\n),\\nfull_text as (\\n    select id,\\n           ts_rank_cd(to_tsvector(\\'english\\', title), websearch_to_tsquery($query_text)) as rank_score,\\n           row_number() over (order by ts_rank_cd(to_tsvector(\\'english\\', title), websearch_to_tsquery($query_text)) desc) as rank_ix\\n    from documents\\n    where to_tsvector(\\'english\\', title) @@ websearch_to_tsquery($query_text)\\n    order by rank_ix\\n    limit least($match_count, 30)\\n),\\nsemantic as (\\n    select id,\\n           row_number() over (order by embedding <#> $query_embedding) as rank_ix\\n    from documents\\n    order by rank_ix\\n    limit least($match_count, 30)\\n)\\nselect documents.*\\nfrom fuzzy\\nfull outer join full_text on fuzzy.id = full_text.id\\nfull outer join semantic on coalesce(fuzzy.id, full_text.id) = semantic.id\\njoin documents on coalesce(fuzzy.id, full_text.id, semantic.id) = documents.id\\norder by\\n    coalesce(1.0 / ($rrf_k + fuzzy.rank_ix), 0.0) * $fuzzy_weight +\\n    coalesce(1.0 / ($rrf_k + full_text.rank_ix), 0.0) * $full_text_weight +\\n    coalesce(1.0 / ($rrf_k + semantic.rank_ix), 0.0) * $semantic_weight\\ndesc\\nlimit least($match_count, 30);\\n\\nDebugging the rankings\\nWhen getting the results back, it’s very useful to understand why something matched and not. First, we need to ensure we’re returning all scores from the various CTEs.\\nsemantic as (\\n  select id,\\n  1 - (embedding <=> $query_embedding) as cosine_similarity, \\n)\\n\\nNext, we need to actually include it in the final response. I’ve found it useful to store as a JSON object that you can pass around however you want.\\nselect\\n  ...\\n  json_build_object(\\n      \\'fuzzy\\', json_build_object(\\'rank_ix\\', fuzzy.rank_ix, \\'sim_score\\', fuzzy.sim_score),\\n      \\'full_text\\', json_build_object(\\'rank_ix\\', full_text.rank_ix, \\'rank_score\\', full_text.rank_score),\\n      \\'semantic\\', json_build_object(\\'rank_ix\\', semantic.rank_ix, \\'cosine_similarity\\', semantic.cosine_similarity)\\n  ) as rankings\\n...\\n\\nHere’s what it’ll look like:\\n{\\n  \"rankings\": {\\n    \"fuzzy\": {\\n      \"rank_ix\": 5,\\n      \"sim_score\": 0.6\\n    },\\n    \"full_text\": {\\n      \"rank_ix\": 4,\\n      \"rank_score\": 0.756\\n    },\\n    \"semantic\": {\\n      \"rank_ix\": 1,\\n      \"cosine_similarity\": 0.912\\n    }\\n  }\\n}\\n\\nTuning full text search\\nWeighing tsvectors\\nNow, your actual documents might actually include some content and not just a title. Let’s go ahead and add a body column.\\ncreate table documents (\\n    id bigint primary key generated always as identity,\\n    title text,\\n    body text, \\n    fts_title tsvector generated always as (to_tsvector(\\'english\\', title)) stored, \\n    fts_body tsvector generated always as (to_tsvector(\\'english\\', body)) stored, \\n    embedding vector(1536)\\n);\\n\\nAs you can see, we’ll only keep a single embedding column even though we have multiple fields. Personally, I’ve found no significant performance in keeping multiple embeddings, but instead keeping both title and body in the same. After all, the title should™️ be a short representation of the body. I encourage you to experiment with this based on your needs.\\nNow, let’s look at the fts_ columns. We expect the title to be short and keyword-rich, while the body will be longer and include more details. Thus we need to adjust how the full text search columns are weighed against each other. Read the docs to get a better understanding here: 12.3.3. Ranking Search Results. Here’s the tl;dr:\\n\\nWeights allow prioritizing words based on their location or importance in the document.\\nA-weight: Most important (e.g., title, headers). Default 1.0\\nB-weight: Important (e.g., beginning of document, abstract). Default 0.4\\nC-weight: Standard importance (e.g., main body text). Default 0.2\\nD-weight: Least important (e.g., footnotes, annotations). Default 0.1\\nAdjust weights to fine-tune relevance based on document structure and application needs.\\n\\nThe title is given more weight than the body because it’s typically a concise representation of the document’s main topic. Users often scan titles first when searching, so matching keywords in the title are usually more relevant to the user’s intent than matches in the body text.\\nWith this information, we can go ahead and update the indexes:\\ncreate table documents (\\n    id bigint primary key generated always as identity,\\n    title text,\\n    body text,\\n    fts_title tsvector generated always as (setweight(to_tsvector(\\'english\\', coalesce(title, \\'\\')), \\'A\\')) stored, \\n    fts_body tsvector generated always as (setweight(to_tsvector(\\'english\\', coalesce(body, \\'\\')), \\'C\\')) stored, \\n    embedding vector(1536)\\n);\\n\\nThis will make the title have a weight of 1.0 and body a weight of 0.2.\\nLike before, we’ll add the new fts_body to the final query. I’ve also renamed the previous full_text to fts_title.\\n...\\nfts_body as (\\n    select id,\\n           ts_rank_cd(fts_body, websearch_to_tsquery($query_text)) as rank_score,\\n           row_number() over (order by ts_rank_cd(fts_body, websearch_to_tsquery($query_text)) desc) as rank_ix\\n    from documents\\n    where fts_body @@ websearch_to_tsquery($query_text)\\n    order by rank_ix\\n    limit least($match_count, 30)\\n),\\n...\\n\\nAnd the combined query\\nselect\\n    documents.*,\\n    coalesce(1.0 / ($rrf_k + fuzzy.rank_ix), 0.0) * $fuzzy_weight +\\n    coalesce(1.0 / ($rrf_k + fts_title.rank_ix), 0.0) * $fts_title_weight +\\n    coalesce(1.0 / ($rrf_k + fts_body.rank_ix), 0.0) * $fts_body_weight +\\n    coalesce(1.0 / ($rrf_k + semantic.rank_ix), 0.0) * $semantic_weight as combined_rank,\\n    json_build_object(\\n        \\'fuzzy\\', json_build_object(\\'rank_ix\\', fuzzy.rank_ix, \\'sim_score\\', fuzzy.sim_score),\\n        \\'fts_title\\', json_build_object(\\'rank_ix\\', fts_title.rank_ix, \\'rank_score\\', fts_title.rank_score),\\n        \\'fts_body\\', json_build_object(\\'rank_ix\\', fts_body.rank_ix, \\'rank_score\\', fts_body.rank_score), \\n        \\'semantic\\', json_build_object(\\'rank_ix\\', semantic.rank_ix, \\'cosine_similarity\\', semantic.cosine_similarity)\\n    ) as debug_rankings\\nfrom fuzzy\\nfull outer join fts_title on fuzzy.id = fts_title.id\\nfull outer join fts_body on coalesce(fuzzy.id, fts_title.id) = fts_body.id\\nfull outer join semantic on coalesce(fuzzy.id, fts_title.id, fts_body.id) = semantic.id\\njoin documents on coalesce(fuzzy.id, fts_title.id, fts_body.id, semantic.id) = documents.id\\norder by combined_rank desc\\nlimit least($match_count, 30);\\n\\nAdjusting for length\\nIf you read the documentation for ts_rank_cd you saw that there’s a normalization parameter. If not, here it is:\\n\\nBoth ranking functions take an integer normalization option that specifies whether and how a document’s length should impact its rank. The integer option controls several behaviors, so it is a bit mask: you can specify one or more behaviors using | (for example, 2|4).\\n\\nWe can use these various options to:\\n\\nAdjust for document length bias\\nBalance relevance across diverse document sets\\nScale ranking results for consistent presentation\\n\\n\\nOption ValueWhen to UseUse CaseNo normalization (0)When you want raw ranking scores without adjustmentsComparing documents of similar length and structureLog length normalization (1)When you want to mildly reduce the impact of document lengthMixed-length documents where longer docs shouldn’t dominateLength normalization (2)When you want to strongly normalize by document lengthDiverse document lengths where content density matters more than total matchesHarmonic mean distance (ts_rank_cd only) (4)When you want to consider term proximity in rankingPhrases or closely related terms are important in your searchUnique word normalization (8)When you want to favor documents with more diverse vocabularyRewarding content richness over repetitionLog unique word normalization (16)When you want to mildly reduce the impact of vocabulary diversityBalancing between vocabulary richness and raw term frequencyScaling to 0-1 range (32)When you need a consistent score range for all queriesDisplaying scores as percentages or progress bars\\nCombine options using bitwise OR (|) for more nuanced normalization. For example:\\nUse 2|4 to normalize by both length and term proximity\\nUse 1|8 for a balanced approach considering both document length and vocabulary diversity\\nI’ve found good results with setting 0 (no normalization) for the title and 1 (logarithmic doc length) for the body. Again, I encourage you to experiment with different options to find the best fit for your use case.\\nReranking with cross-encoder\\nMany retrieval systems are two-step based. This means you’ll use a bi-directional encoder to retrieve the initial N results, then a cross-encoder to rank these against the search query.\\nThe bi-encoder is fast, making it great for searching a multitude of documents. The cross-encoder is slower but more performant, making it great for reranking retrieved results.\\nCross-encoders differ from bi-encoders in that they process the query and document together, allowing for more nuanced understanding of the relationship between them. This results in better ranking accuracy but at the cost of computation time and scalability. Here’s a simple diagram to illustrate the process:\\nFTSdocumentsSemantic SearchdocumentsFuzzydocumentsRerank with Cross-encoder modelFinal documents\\nThere are a bunch of different tools out there to do this. One of the best is Cohere’s Rerank. Another way is to build your own with GPT from OpenAI.\\nBoosting results to improve UX\\nTo provide an even better user experience, you might want to boost certain columns depending on your use case. For example, in a knowledge search, you might want to boost search results that the user has interacted with. You also might want to boost recently changed results.\\nBoosting recent results or user-specific results can improve UX because it personalizes the search experience. Recent results are often more relevant, especially for time-sensitive information. User-specific boosts can surface content the user has previously found useful, making their search more efficient.\\nAfter BoostingBefore Boosting1. Result C2. Result A3. Result E4. Result B5. Result D1. Result A2. Result B3. Result C4. Result D5. Result E\\nAssuming we have updated_at and updated_by columns in our documents table, we could implement it like this:\\nselect\\n    ...\\n    -- Recency boost\\n    (1 + $recency_boost * (1 - extract(epoch from (now() - documents.updated_at)) / extract(epoch from (now() - \\'2020-01-01\\'::timestamp)))) *\\n    -- User boost\\n    case when documents.updated_by = $current_user_id then (1 + $user_boost) else 1 end\\n    as combined_rank,\\n    json_build_object(\\n        ...\\n        \\'recency_boost\\', (1 - extract(epoch from (now() - documents.updated_at)) / extract(epoch from (now() - \\'2020-01-01\\'::timestamp))),\\n        \\'user_boost\\', case when documents.updated_by = $current_user_id then $user_boost else 0 end\\n    ) as debug_rankings\\n\\nWhen should I look for alternative solutions?\\nWhile Postgres is a solid choice for many search scenarios, it’s not without limitations. The lack of advanced algorithms like BM25 can be felt when dealing with diverse document lengths. Postgres’s reliance on TF-IDF for full-text search can struggle with very long documents and rare terms in large collections.\\nEdit: Postgres actually doesn’t use TF-IDF for full-text search. Its built-in ranking functions (ts_rank and ts_rank_cd) primarily consider term frequency within individual documents and term proximity, but don’t take into account corpus-wide statistics. This approach can still struggle with very long documents and doesn’t inherently account for the rarity of terms across the entire collection.\\nMake sure to measure before you look into alternative solutions. Chances are, it might not be worth it.\\nBonus: adding BM25\\nUnlike Postgres’s FTS, BM25 considers corpus-wide statistics and document length normalization. Here are some of the reaons you might want to use it:\\n\\nBetter handling of document length variations\\nImproved relevance for rare terms\\nAccounts for diminishing returns of term frequency\\nIndustry standard in information retrieval\\n\\nAnd here are some extensions you can install go get BM25 going:\\n\\npg_search\\npg_bestmatch.rs\\n\\nConclusion\\nWe’ve covered a lot of ground in this post, from basic full-text search to advanced techniques like fuzzy matching, semantic search, and result boosting. By leveraging Postgres’s powerful features, you can create a robust and flexible search engine tailored to your specific needs.\\nPostgres might not be the first tool that comes to mind for search, but it gets you really far.\\nRemember, the key to a great search experience is continuous iteration and fine-tuning. Use the debugging techniques we discussed to understand how your search is performing, and don’t be afraid to adjust weights and parameters based on user feedback and behavior.\\nLet me know if I missed anything! :)      Further reading     How we systematically analyze user feedback with LLMs   2024-07-21      The idea of internal no-code platforms   2024-06-29      Postgres is all you need, even for vectors   2024-06-15            Content     Table with four legs     Implementing fuzzy search     Debugging the rankings     Tuning full text search     Weighing tsvectors     Adjusting for length     Reranking with cross-encoder     Boosting results to improve UX     When should I look for alternative solutions?     Bonus: adding BM25     Conclusion      Don\\'t miss the next post    Get some nuggets \\nFollow on\\n X            \\nwritten by eric zakariasson (not an LLM)       X      GitHub      LinkedIn          ',\n",
       "  'title': 'Postgres as a Search Engine',\n",
       "  'url': 'https://anyblockers.com/posts/postgres-as-a-search-engine'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://gregsramblings.com/stupid-but-useful-ai-tricks-creating-calendar-entries-from-an-image-using-anthropic-claude-35\n",
      "Summary: A user used Anthropic Claude 3.5 to create an iCalendar file (.ics) from a JPG image of their jazz piano lesson schedule, saving them the hassle of manually entering each appointment into Google Calendar.\n",
      "-------------\n",
      "The text discusses the importance of thinking in graphs rather than lists when it comes to network security. It highlights how attackers can use visualizations and connections between systems to compromise a High Value Asset (HVA) by targeting dependent elements, such as terminal servers, admin accounts, and certificate authorities.\n",
      "\n",
      "To protect against this, defenders should:\n",
      "\n",
      "1. Visualize their network as a graph.\n",
      "2. Implement controls to prune the graph, such as:\n",
      "\t* Reducing unwanted edges that create huge connectivity bursts.\n",
      "\t* Minimizing the number of admins using Just-In-Time/Just Enough techniques.\n",
      "\t* Using two-factor authentication.\n",
      "\t* Applying solid credential rotation approaches.\n",
      "3. Rethink forest trust relationships.\n",
      "\n",
      "The text also emphasizes the need for defenders to manage their graph, rather than just thinking in lists, and to learn from attackers' methods of understanding the network. It provides further reading on attack graphs and related papers.\n",
      "\n",
      "Key takeaways:\n",
      "\n",
      "* Attackers think in graphs, not lists.\n",
      "* Defenders should visualize their network as a graph.\n",
      "* Implementing controls to prune the graph can help protect against attacks.\n",
      "* Minimizing admin accounts, using two-factor authentication, and applying solid credential rotation approaches are essential.\n",
      "* Rethinking forest trust relationships is also crucial.\n",
      "-------------\n",
      "The article discusses using Postgres as a search engine, covering various techniques to improve search results. Here's a summary of the key points:\n",
      "\n",
      "1. **Basic Full-Text Search**: Postgres provides basic full-text search capabilities through its FTS (Full-Text Search) module.\n",
      "2. **Fuzzy Matching**: To improve search accuracy, you can use fuzzy matching techniques like Levenshtein distance or Jaro-Winkler distance to find documents with similar spellings.\n",
      "3. **Semantic Search**: Use a cross-encoder model to query and document together, allowing for more nuanced understanding of the relationship between them.\n",
      "4. **Result Boosting**: Boost certain columns depending on your use case, such as boosting search results that the user has interacted with or recently changed results.\n",
      "5. **Debugging Techniques**: Use debugging techniques like measuring before looking into alternative solutions and adjusting weights and parameters based on user feedback and behavior.\n",
      "6. **Alternative Solutions**: Consider using alternative solutions like BM25 (Best Match) when Postgres's FTS struggles with diverse document lengths or rare terms in large collections.\n",
      "\n",
      "The article also provides a simple diagram to illustrate the process of semantic search and result boosting, as well as code examples for implementing these techniques in Postgres.\n",
      "\n",
      "Some key takeaways from the article are:\n",
      "\n",
      "* Postgres can be used as a robust and flexible search engine with advanced features like fuzzy matching and semantic search.\n",
      "* Result boosting can improve UX by personalizing the search experience based on user interactions or recent changes.\n",
      "* Debugging techniques are essential to understand how your search is performing and make adjustments accordingly.\n",
      "\n",
      "Overall, the article provides a comprehensive guide to using Postgres as a search engine, covering various techniques to improve search results and provide a better user experience.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summarizer = ArticleSummarizer()\n",
    "summaries = summarizer.summarize_articles(articles)\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Create Calendar Entries with Anthropic Claude 3.5Greg’s RamblingsFollowGreg’s RamblingsFollowStupid but useful AI tricks: Creating calendar entries from an image using Anthropic Claude 3.5Greg Wilson·Aug 25, 2024·4 min readA few days ago, my jazz piano teacher sent me the new fall/winter schedule for my private jazz piano lessons -- 13 different dates -- as a JPG (mine are outlined in green marker):\\n\\nI was too lazy to go make 13 entries in Google Calendar, so I decided to see if Claude could help me out:\\nI first uploaded the jpg to Claude 3.5 Sonnet)...\\n\\nMy prompt: List the dates that are outlined in green\\nThe dates outlined in green in the image are:\\nFriday, 13-Sep-24 Friday, 27-Sep-24 Friday, 11-Oct-24 Friday, 8-Nov-24 Friday, 6-Dec-24 Friday, 20-Dec-24 Friday, 17-Jan-25 Friday, 31-Jan-25 Friday, 28-Feb-25 Friday, 14-Mar-25 Friday, 11-Apr-25 Friday, 25-Apr-25 Friday, 23-May-25\\nThese dates correspond to the Friday column for students who have their first every-other-week lesson starting the week of Tuesday Sept 10, 2024.\\n\\nCool - that was easy and accurate. Now I need to get it into my calendar, so I asked it to create an ics file...\\n\\nMy prompt: Create a calendar ics file that creates an appointment on each of those dates at 2pm pacific time. The appointment title will be “Jazz Piano Lesson”\\nCertainly. I\\'ll create an ICS (iCalendar) file with appointments for \"Jazz Piano Lesson\" on each of the dates you specified, set for 2:00 PM Pacific Time.\\n\\nBEGIN:VCALENDAR\\nVERSION:2.0\\nPRODID:-//Claude//Jazz Piano Lessons//EN\\nCALSCALE:GREGORIAN\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20240913T210000Z\\nDTEND:20240913T220000Z\\nDTSTAMP:20240824T000000Z\\nUID:20240913jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20240927T210000Z\\nDTEND:20240927T220000Z\\nDTSTAMP:20240824T000000Z\\nUID:20240927jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20241011T210000Z\\nDTEND:20241011T220000Z\\nDTSTAMP:20240824T000000Z\\nUID:20241011jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20241108T220000Z\\nDTEND:20241108T230000Z\\nDTSTAMP:20240824T000000Z\\nUID:20241108jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20241206T220000Z\\nDTEND:20241206T230000Z\\nDTSTAMP:20240824T000000Z\\nUID:20241206jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20241220T220000Z\\nDTEND:20241220T230000Z\\nDTSTAMP:20240824T000000Z\\nUID:20241220jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20250117T220000Z\\nDTEND:20250117T230000Z\\nDTSTAMP:20240824T000000Z\\nUID:20250117jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20250131T220000Z\\nDTEND:20250131T230000Z\\nDTSTAMP:20240824T000000Z\\nUID:20250131jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20250228T220000Z\\nDTEND:20250228T230000Z\\nDTSTAMP:20240824T000000Z\\nUID:20250228jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20250314T210000Z\\nDTEND:20250314T220000Z\\nDTSTAMP:20240824T000000Z\\nUID:20250314jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20250411T210000Z\\nDTEND:20250411T220000Z\\nDTSTAMP:20240824T000000Z\\nUID:20250411jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20250425T210000Z\\nDTEND:20250425T220000Z\\nDTSTAMP:20240824T000000Z\\nUID:20250425jazzpiano@claude.ai\\nEND:VEVENT\\nBEGIN:VEVENT\\nSUMMARY:Jazz Piano Lesson\\nDTSTART:20250523T210000Z\\nDTEND:20250523T220000Z\\nDTSTAMP:20240824T000000Z\\nUID:20250523jazzpiano@claude.ai\\nEND:VEVENT\\nEND:VCALENDAR\\n\\n\\nI\\'ve created an ICS file with the following details:\\n\\nEach event is titled \"Jazz Piano Lesson\"\\n\\nThe events are scheduled for 2:00 PM Pacific Time on each of the specified dates\\n\\nEach event is set to last for one hour\\n\\nThe time zone is properly adjusted to UTC (Z) in the file\\n\\nUnique identifiers (UIDs) are provided for each event\\n\\n\\nYou can save this content as a .ics file and import it into most calendar applications. Here are a few notes about the file:\\n\\nThe times are in UTC (Coordinated Universal Time). 2:00 PM Pacific Time translates to 9:00 PM UTC (21:00) for dates not in Daylight Saving Time, and 10:00 PM UTC (22:00) for dates within Daylight Saving Time. The file accounts for these changes.\\n\\nThe DTSTAMP for all events is set to the date of this conversation (August 24, 2024) as a placeholder. Your calendar application will likely update this when you import the events.\\n\\nThe UID for each event is unique, based on the date of the lesson and a simple string.\\n\\n\\nTo use this file:\\n\\nCopy the content of the artifact\\n\\nPaste it into a new text file\\n\\nSave the file with a .ics extension (e.g., \"jazz_piano_lessons.ics\")\\n\\nImport this file into your preferred calendar application\\n\\n\\nMost calendar applications should be able to read this file and create the appropriate recurring events. Let me know if you need any modifications or have any questions!\\n\\nI went to calendar.google.com, imported the ics, and my appointments were imported perfectly!(To import ics files into Google calendar, go to the calendar settings, then to import/export)\\nI\\'m pleasantly surprised at how easy this was!\\nI tried the exact same thing in ChatGPT -- it identified the dates in the images, but it was unable to create an .ics file. However, it did give me some Python code to create the ics file for me.\\nicalendarclaude.aicalendarimage processing\\xa0Share this',\n",
       "  'title': 'Anthropic Claude 3.5 can create icalendar files, so I did this',\n",
       "  'url': 'https://gregsramblings.com/stupid-but-useful-ai-tricks-creating-calendar-entries-from-an-image-using-anthropic-claude-35'},\n",
       " {'content': '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nShared/Defenders think in lists. Attackers think in graphs. As long as this is true, attackers win.md at master · JohnLaTwC/Shared · GitHub\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Sign in\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n        Product\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nActions\\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\nPackages\\n        Host and manage packages\\n      \\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\nCodespaces\\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\nGitHub Copilot\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\nCode review\\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\nIssues\\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\nDiscussions\\n        Collaborate outside of code\\n      \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      All features\\n\\n    \\n\\n\\n\\n      Documentation\\n\\n    \\n\\n\\n\\n\\n\\n      GitHub Skills\\n\\n    \\n\\n\\n\\n\\n\\n      Blog\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Solutions\\n        \\n\\n\\n\\n\\n\\nBy size\\n\\n\\n\\n      Enterprise\\n\\n    \\n\\n\\n\\n      Teams\\n\\n    \\n\\n\\n\\n      Startups\\n\\n    \\n\\n\\n\\n\\nBy industry\\n\\n\\n\\n      Healthcare\\n\\n    \\n\\n\\n\\n      Financial services\\n\\n    \\n\\n\\n\\n      Manufacturing\\n\\n    \\n\\n\\n\\n\\nBy use case\\n\\n\\n\\n      CI/CD & Automation\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      DevSecOps\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n        Resources\\n        \\n\\n\\n\\n\\n\\nTopics\\n\\n\\n\\n      AI\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      Security\\n\\n    \\n\\n\\n\\n      Software Development\\n\\n    \\n\\n\\n\\n      View all\\n\\n    \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      Learning Pathways\\n\\n    \\n\\n\\n\\n\\n\\n      White papers, Ebooks, Webinars\\n\\n    \\n\\n\\n\\n\\n\\n      Customer Stories\\n\\n    \\n\\n\\n\\n      Partners\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Open Source\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub Sponsors\\n        Fund open source developers\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nThe ReadME Project\\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n        Enterprise\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnterprise platform\\n        AI-powered developer platform\\n      \\n\\n\\n\\n\\nAvailable add-ons\\n\\n\\n\\n\\n\\n\\n\\nAdvanced Security\\n        Enterprise-grade security features\\n      \\n\\n\\n\\n\\n\\n\\n\\nGitHub Copilot\\n        Enterprise-grade AI features\\n      \\n\\n\\n\\n\\n\\n\\n\\nPremium Support\\n        Enterprise-grade 24/7 support\\n      \\n\\n\\n\\n\\n\\n\\nPricing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClear\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n              Search syntax tips\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nQuery\\n\\n\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n\\n\\n\\n\\n     Cancel\\n\\n    Create saved search\\n\\n\\n\\n\\n\\n\\n\\n\\n                Sign in\\n              \\n\\n\\n                Sign up\\n              \\nReseting focus\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        JohnLaTwC\\n \\n/\\n\\nShared\\n\\nPublic\\n\\n\\n\\n\\n\\n \\n\\nNotifications\\n You must be signed in to change notification settings\\n\\n\\n \\n\\nFork\\n    58\\n\\n\\n\\n\\n \\n\\n\\n          Star\\n 420\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCode\\n\\n\\n\\n\\n\\n\\n\\nIssues\\n0\\n\\n\\n\\n\\n\\n\\nPull requests\\n1\\n\\n\\n\\n\\n\\n\\nActions\\n\\n\\n\\n\\n\\n\\n\\nProjects\\n0\\n\\n\\n\\n\\n\\n\\nSecurity\\n\\n\\n\\n\\n\\n\\n\\nInsights\\n\\n\\n\\n \\n\\n \\n\\n\\nAdditional navigation options\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Code\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Issues\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Pull requests\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Actions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Projects\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Security\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Insights\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFooter\\n\\n\\n\\n\\n\\n\\n\\n\\n        © 2024 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n\\n\\n      Manage cookies\\n    \\n\\n\\n\\n\\n\\n      Do not share my personal information\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    You can’t perform that action at this time.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "  'title': 'Defenders think in lists, attackers think in graphs (2015)',\n",
       "  'url': 'https://github.com/JohnLaTwC/Shared/blob/master/Defenders%20think%20in%20lists.%20Attackers%20think%20in%20graphs.%20As%20long%20as%20this%20is%20true%2C%20attackers%20win.md'},\n",
       " {'content': ' Postgres as a search engine\\n          any blockers?         \\nSubscribe\\n             Last Monday  \\n•\\n 15 minute read\\n  Postgres as a search engine Build a retrieval system with semantic, full-text, and fuzzy search in Postgres to be used as a backbone in RAG pipelines.  Search is hard. It’s a critical part of many apps, but getting it right isn’t easy. This is especially true for RAG-pipelines where the quality of retrieval can make or break the entire process.\\nWhile semantic search is trendy, good old lexical search is still the backbone. Semantic techniques can improve results, but they work best when added to a solid text-based search foundation.\\nIn this post, we’ll explore how to use Postgres to create a robust search engine. We’ll combine three techniques:\\n\\nFull-text search with tsvector\\nSemantic search with pgvector\\nFuzzy matching with pg_trgm\\nBonus: BM25\\n\\nThis approach might not be the absolute best for every situation, but it’s a great alternative to setting up a separate search service. It’s a solid starting point that you can implement and scale within your existing Postgres database.\\nI won’t go into why you should Just Use Postgres™️ for everything, but if you want to read about it, here are a couple of good resources:\\n\\nPostgres is all you need, even for vectors\\nJust Use Postgres for Everything\\nPostgreSQL is Enough\\n\\nTable with four legs\\nThis is the table we’ll work with as an example.\\ncreate table documents (\\n    id bigint primary key generated always as identity,\\n    title text,\\n    fts_title tsvector generated always as (to_tsvector(\\'english\\', title)) stored,\\n    embedding vector(1536)\\n);\\n\\nWe’ll evolve this table down the road.\\nSince Supabase has a great article on implementing hybrid search, we’ll use that as a starting point. Hybrid search\\n\\nFollow the guide to implement FTS with GIN-indexes and semantic search with pgvector (also known as bi-encoder dense retrieval).\\nFrom personal experience, I’d opt for 1536 dimensional embeddings since it’s gotten me way better results. Source: trust me.\\nI’ve replaced the Supabase function with just CTEs and a query, as well as prefixed the params with $. Here’s what it’ll look like:\\n\\nwith full_text as (\\n    select\\n        id,\\n        -- Note: ts_rank_cd is not indexable but will only rank matches of the where clause\\n        -- which shouldn\\'t be too big\\n        row_number() over(order by ts_rank_cd(fts_title, websearch_to_tsquery($query_text)) desc) as rank_ix\\n    from\\n        documents\\n    where\\n        fts_title @@ websearch_to_tsquery($query_text)\\n    order by rank_ix\\n    limit least($match_count, 30)\\n),\\nsemantic as (\\n    select\\n        id,\\n        row_number() over (order by embedding <#> $query_embedding) as rank_ix\\n    from\\n        documents\\n    order by rank_ix\\n    limit least($match_count, 30)\\n)\\nselect\\n    documents.*\\nfrom\\n    full_text\\n    full outer join semantic\\n        on full_text.id = semantic.id\\n    join documents\\n        on coalesce(full_text.id, semantic.id) = documents.id\\norder by\\n    coalesce(1.0 / ($rrf_k + full_text.rank_ix), 0.0) * $full_text_weight +\\n    coalesce(1.0 / ($rrf_k + semantic.rank_ix), 0.0) * $semantic_weight\\n    desc\\nlimit\\n    least($match_count, 30);\\n\\nNote: We’re using coalesce in several places for important reasons:\\n\\n\\nIn the join clause:\\njoin documents\\n    on coalesce(full_text.id, semantic.id) = documents.id\\n\\nThis ensures that we include results from both full-text and semantic searches, even if a document appears in only one of these result sets. If a document is found by full-text search but not by semantic search (or vice versa), we still want to include it.\\n\\n\\nIn the order by clause:\\ncoalesce(1.0 / ($rrf_k + full_text.rank_ix), 0.0) * $full_text_weight +\\ncoalesce(1.0 / ($rrf_k + semantic.rank_ix), 0.0) * $semantic_weight\\n\\nThis handles cases where a document might be present in one search result but not the other. If a document isn’t in the full-text results, its full_text.rank_ix will be NULL, so we use coalesce to treat it as 0.0 in the ranking calculation. The same applies for semantic search results.\\n\\n\\nHere we’re using Reciprocal Ranked Fusion (RRF) to merge the results.\\nList 3List 2List 1CABBCAABCRRFFinal RankingABC\\n\\nThis method ensures that items ranked high in multiple lists are given a high rank in the final list. It also ensures that items ranked high in only a few lists but low in others are not given a high rank in the final list. Placing the rank in the denominator when calculating score helps penalize the low ranking records.\\n\\nIt’s also worth noting:\\n\\n$rrf_k: To prevent extremely high scores for items ranked first (since we’re dividing by the rank), a k constant is often added to the denominator to smooth the score.\\n$ _weight: We can assign a weight to each method. This is very useful when you’re tuning the results.\\n\\nImplementing fuzzy search\\nWhile this gets us a long way, an immediate issue will be typos in named entities. While semantic search eliminates some of these issues by capturing similarity, it struggles to do so for names, acronyms, and other text that’s not semantically similar. To mitigate this, we’ll introduce the pg_trgm extension to allow for fuzzy searching.\\ncreate extension if not exists pg_trgm;\\n\\nIt operates with Trigrams. Here’s how it works:\\n\\nTrigrams are useful for fuzzy search because they break down words into three-character sequences. This allows for matching similar words even if they contain typos or slight variations. For example, “hello” and “helo” share many trigrams, making them easier to match in a fuzzy search.\\nYou have to create a new index for the desired column like this:\\ncreate index idx_documents_title_trgm on documents using gin (title gin_trgm_ops);\\n\\nAfter that, you need to add it to the full search query. The extension exposes the % operator to filter out text where similarity is larger than pg_trgm.similarity_threshold (default is 0.3). There are also several other operators that are useful. Everything is well documented here: pg_trgm — support for similarity of text using trigram matching\\nHere’s the new query with fuzzy searching implemented:\\nwith fuzzy as ( \\n    select id,\\n           similarity(title, $query_text) as sim_score,\\n           row_number() over (order by similarity(title, $query_text) desc) as rank_ix\\n    from documents\\n    where title % $query_text\\n    order by rank_ix\\n    limit least($match_count, 30)\\n),\\nfull_text as (\\n    select id,\\n           ts_rank_cd(to_tsvector(\\'english\\', title), websearch_to_tsquery($query_text)) as rank_score,\\n           row_number() over (order by ts_rank_cd(to_tsvector(\\'english\\', title), websearch_to_tsquery($query_text)) desc) as rank_ix\\n    from documents\\n    where to_tsvector(\\'english\\', title) @@ websearch_to_tsquery($query_text)\\n    order by rank_ix\\n    limit least($match_count, 30)\\n),\\nsemantic as (\\n    select id,\\n           row_number() over (order by embedding <#> $query_embedding) as rank_ix\\n    from documents\\n    order by rank_ix\\n    limit least($match_count, 30)\\n)\\nselect documents.*\\nfrom fuzzy\\nfull outer join full_text on fuzzy.id = full_text.id\\nfull outer join semantic on coalesce(fuzzy.id, full_text.id) = semantic.id\\njoin documents on coalesce(fuzzy.id, full_text.id, semantic.id) = documents.id\\norder by\\n    coalesce(1.0 / ($rrf_k + fuzzy.rank_ix), 0.0) * $fuzzy_weight +\\n    coalesce(1.0 / ($rrf_k + full_text.rank_ix), 0.0) * $full_text_weight +\\n    coalesce(1.0 / ($rrf_k + semantic.rank_ix), 0.0) * $semantic_weight\\ndesc\\nlimit least($match_count, 30);\\n\\nDebugging the rankings\\nWhen getting the results back, it’s very useful to understand why something matched and not. First, we need to ensure we’re returning all scores from the various CTEs.\\nsemantic as (\\n  select id,\\n  1 - (embedding <=> $query_embedding) as cosine_similarity, \\n)\\n\\nNext, we need to actually include it in the final response. I’ve found it useful to store as a JSON object that you can pass around however you want.\\nselect\\n  ...\\n  json_build_object(\\n      \\'fuzzy\\', json_build_object(\\'rank_ix\\', fuzzy.rank_ix, \\'sim_score\\', fuzzy.sim_score),\\n      \\'full_text\\', json_build_object(\\'rank_ix\\', full_text.rank_ix, \\'rank_score\\', full_text.rank_score),\\n      \\'semantic\\', json_build_object(\\'rank_ix\\', semantic.rank_ix, \\'cosine_similarity\\', semantic.cosine_similarity)\\n  ) as rankings\\n...\\n\\nHere’s what it’ll look like:\\n{\\n  \"rankings\": {\\n    \"fuzzy\": {\\n      \"rank_ix\": 5,\\n      \"sim_score\": 0.6\\n    },\\n    \"full_text\": {\\n      \"rank_ix\": 4,\\n      \"rank_score\": 0.756\\n    },\\n    \"semantic\": {\\n      \"rank_ix\": 1,\\n      \"cosine_similarity\": 0.912\\n    }\\n  }\\n}\\n\\nTuning full text search\\nWeighing tsvectors\\nNow, your actual documents might actually include some content and not just a title. Let’s go ahead and add a body column.\\ncreate table documents (\\n    id bigint primary key generated always as identity,\\n    title text,\\n    body text, \\n    fts_title tsvector generated always as (to_tsvector(\\'english\\', title)) stored, \\n    fts_body tsvector generated always as (to_tsvector(\\'english\\', body)) stored, \\n    embedding vector(1536)\\n);\\n\\nAs you can see, we’ll only keep a single embedding column even though we have multiple fields. Personally, I’ve found no significant performance in keeping multiple embeddings, but instead keeping both title and body in the same. After all, the title should™️ be a short representation of the body. I encourage you to experiment with this based on your needs.\\nNow, let’s look at the fts_ columns. We expect the title to be short and keyword-rich, while the body will be longer and include more details. Thus we need to adjust how the full text search columns are weighed against each other. Read the docs to get a better understanding here: 12.3.3. Ranking Search Results. Here’s the tl;dr:\\n\\nWeights allow prioritizing words based on their location or importance in the document.\\nA-weight: Most important (e.g., title, headers). Default 1.0\\nB-weight: Important (e.g., beginning of document, abstract). Default 0.4\\nC-weight: Standard importance (e.g., main body text). Default 0.2\\nD-weight: Least important (e.g., footnotes, annotations). Default 0.1\\nAdjust weights to fine-tune relevance based on document structure and application needs.\\n\\nThe title is given more weight than the body because it’s typically a concise representation of the document’s main topic. Users often scan titles first when searching, so matching keywords in the title are usually more relevant to the user’s intent than matches in the body text.\\nWith this information, we can go ahead and update the indexes:\\ncreate table documents (\\n    id bigint primary key generated always as identity,\\n    title text,\\n    body text,\\n    fts_title tsvector generated always as (setweight(to_tsvector(\\'english\\', coalesce(title, \\'\\')), \\'A\\')) stored, \\n    fts_body tsvector generated always as (setweight(to_tsvector(\\'english\\', coalesce(body, \\'\\')), \\'C\\')) stored, \\n    embedding vector(1536)\\n);\\n\\nThis will make the title have a weight of 1.0 and body a weight of 0.2.\\nLike before, we’ll add the new fts_body to the final query. I’ve also renamed the previous full_text to fts_title.\\n...\\nfts_body as (\\n    select id,\\n           ts_rank_cd(fts_body, websearch_to_tsquery($query_text)) as rank_score,\\n           row_number() over (order by ts_rank_cd(fts_body, websearch_to_tsquery($query_text)) desc) as rank_ix\\n    from documents\\n    where fts_body @@ websearch_to_tsquery($query_text)\\n    order by rank_ix\\n    limit least($match_count, 30)\\n),\\n...\\n\\nAnd the combined query\\nselect\\n    documents.*,\\n    coalesce(1.0 / ($rrf_k + fuzzy.rank_ix), 0.0) * $fuzzy_weight +\\n    coalesce(1.0 / ($rrf_k + fts_title.rank_ix), 0.0) * $fts_title_weight +\\n    coalesce(1.0 / ($rrf_k + fts_body.rank_ix), 0.0) * $fts_body_weight +\\n    coalesce(1.0 / ($rrf_k + semantic.rank_ix), 0.0) * $semantic_weight as combined_rank,\\n    json_build_object(\\n        \\'fuzzy\\', json_build_object(\\'rank_ix\\', fuzzy.rank_ix, \\'sim_score\\', fuzzy.sim_score),\\n        \\'fts_title\\', json_build_object(\\'rank_ix\\', fts_title.rank_ix, \\'rank_score\\', fts_title.rank_score),\\n        \\'fts_body\\', json_build_object(\\'rank_ix\\', fts_body.rank_ix, \\'rank_score\\', fts_body.rank_score), \\n        \\'semantic\\', json_build_object(\\'rank_ix\\', semantic.rank_ix, \\'cosine_similarity\\', semantic.cosine_similarity)\\n    ) as debug_rankings\\nfrom fuzzy\\nfull outer join fts_title on fuzzy.id = fts_title.id\\nfull outer join fts_body on coalesce(fuzzy.id, fts_title.id) = fts_body.id\\nfull outer join semantic on coalesce(fuzzy.id, fts_title.id, fts_body.id) = semantic.id\\njoin documents on coalesce(fuzzy.id, fts_title.id, fts_body.id, semantic.id) = documents.id\\norder by combined_rank desc\\nlimit least($match_count, 30);\\n\\nAdjusting for length\\nIf you read the documentation for ts_rank_cd you saw that there’s a normalization parameter. If not, here it is:\\n\\nBoth ranking functions take an integer normalization option that specifies whether and how a document’s length should impact its rank. The integer option controls several behaviors, so it is a bit mask: you can specify one or more behaviors using | (for example, 2|4).\\n\\nWe can use these various options to:\\n\\nAdjust for document length bias\\nBalance relevance across diverse document sets\\nScale ranking results for consistent presentation\\n\\n\\nOption ValueWhen to UseUse CaseNo normalization (0)When you want raw ranking scores without adjustmentsComparing documents of similar length and structureLog length normalization (1)When you want to mildly reduce the impact of document lengthMixed-length documents where longer docs shouldn’t dominateLength normalization (2)When you want to strongly normalize by document lengthDiverse document lengths where content density matters more than total matchesHarmonic mean distance (ts_rank_cd only) (4)When you want to consider term proximity in rankingPhrases or closely related terms are important in your searchUnique word normalization (8)When you want to favor documents with more diverse vocabularyRewarding content richness over repetitionLog unique word normalization (16)When you want to mildly reduce the impact of vocabulary diversityBalancing between vocabulary richness and raw term frequencyScaling to 0-1 range (32)When you need a consistent score range for all queriesDisplaying scores as percentages or progress bars\\nCombine options using bitwise OR (|) for more nuanced normalization. For example:\\nUse 2|4 to normalize by both length and term proximity\\nUse 1|8 for a balanced approach considering both document length and vocabulary diversity\\nI’ve found good results with setting 0 (no normalization) for the title and 1 (logarithmic doc length) for the body. Again, I encourage you to experiment with different options to find the best fit for your use case.\\nReranking with cross-encoder\\nMany retrieval systems are two-step based. This means you’ll use a bi-directional encoder to retrieve the initial N results, then a cross-encoder to rank these against the search query.\\nThe bi-encoder is fast, making it great for searching a multitude of documents. The cross-encoder is slower but more performant, making it great for reranking retrieved results.\\nCross-encoders differ from bi-encoders in that they process the query and document together, allowing for more nuanced understanding of the relationship between them. This results in better ranking accuracy but at the cost of computation time and scalability. Here’s a simple diagram to illustrate the process:\\nFTSdocumentsSemantic SearchdocumentsFuzzydocumentsRerank with Cross-encoder modelFinal documents\\nThere are a bunch of different tools out there to do this. One of the best is Cohere’s Rerank. Another way is to build your own with GPT from OpenAI.\\nBoosting results to improve UX\\nTo provide an even better user experience, you might want to boost certain columns depending on your use case. For example, in a knowledge search, you might want to boost search results that the user has interacted with. You also might want to boost recently changed results.\\nBoosting recent results or user-specific results can improve UX because it personalizes the search experience. Recent results are often more relevant, especially for time-sensitive information. User-specific boosts can surface content the user has previously found useful, making their search more efficient.\\nAfter BoostingBefore Boosting1. Result C2. Result A3. Result E4. Result B5. Result D1. Result A2. Result B3. Result C4. Result D5. Result E\\nAssuming we have updated_at and updated_by columns in our documents table, we could implement it like this:\\nselect\\n    ...\\n    -- Recency boost\\n    (1 + $recency_boost * (1 - extract(epoch from (now() - documents.updated_at)) / extract(epoch from (now() - \\'2020-01-01\\'::timestamp)))) *\\n    -- User boost\\n    case when documents.updated_by = $current_user_id then (1 + $user_boost) else 1 end\\n    as combined_rank,\\n    json_build_object(\\n        ...\\n        \\'recency_boost\\', (1 - extract(epoch from (now() - documents.updated_at)) / extract(epoch from (now() - \\'2020-01-01\\'::timestamp))),\\n        \\'user_boost\\', case when documents.updated_by = $current_user_id then $user_boost else 0 end\\n    ) as debug_rankings\\n\\nWhen should I look for alternative solutions?\\nWhile Postgres is a solid choice for many search scenarios, it’s not without limitations. The lack of advanced algorithms like BM25 can be felt when dealing with diverse document lengths. Postgres’s reliance on TF-IDF for full-text search can struggle with very long documents and rare terms in large collections.\\nEdit: Postgres actually doesn’t use TF-IDF for full-text search. Its built-in ranking functions (ts_rank and ts_rank_cd) primarily consider term frequency within individual documents and term proximity, but don’t take into account corpus-wide statistics. This approach can still struggle with very long documents and doesn’t inherently account for the rarity of terms across the entire collection.\\nMake sure to measure before you look into alternative solutions. Chances are, it might not be worth it.\\nBonus: adding BM25\\nUnlike Postgres’s FTS, BM25 considers corpus-wide statistics and document length normalization. Here are some of the reaons you might want to use it:\\n\\nBetter handling of document length variations\\nImproved relevance for rare terms\\nAccounts for diminishing returns of term frequency\\nIndustry standard in information retrieval\\n\\nAnd here are some extensions you can install go get BM25 going:\\n\\npg_search\\npg_bestmatch.rs\\n\\nConclusion\\nWe’ve covered a lot of ground in this post, from basic full-text search to advanced techniques like fuzzy matching, semantic search, and result boosting. By leveraging Postgres’s powerful features, you can create a robust and flexible search engine tailored to your specific needs.\\nPostgres might not be the first tool that comes to mind for search, but it gets you really far.\\nRemember, the key to a great search experience is continuous iteration and fine-tuning. Use the debugging techniques we discussed to understand how your search is performing, and don’t be afraid to adjust weights and parameters based on user feedback and behavior.\\nLet me know if I missed anything! :)      Further reading     How we systematically analyze user feedback with LLMs   2024-07-21      The idea of internal no-code platforms   2024-06-29      Postgres is all you need, even for vectors   2024-06-15            Content     Table with four legs     Implementing fuzzy search     Debugging the rankings     Tuning full text search     Weighing tsvectors     Adjusting for length     Reranking with cross-encoder     Boosting results to improve UX     When should I look for alternative solutions?     Bonus: adding BM25     Conclusion      Don\\'t miss the next post    Get some nuggets \\nFollow on\\n X            \\nwritten by eric zakariasson (not an LLM)       X      GitHub      LinkedIn          ',\n",
       "  'title': 'Postgres as a Search Engine',\n",
       "  'url': 'https://anyblockers.com/posts/postgres-as-a-search-engine'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://hacker-news.firebaseio.com/v0\"\n",
    "response = requests.get(f\"{base_url}/topstories.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trending_list = response.json()\n",
    "top_k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (prompt_template | llm | StrOutputParser()).invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "class HackernewsNewestFetcher:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://hacker-news.firebaseio.com/v0\"\n",
    "\n",
    "    def fetch_articles(self, top_k: int) -> List[dict]:\n",
    "        response = requests.get(f\"{self.base_url}/topstories.json?print=pretty\")\n",
    "        trending_list = response.json()\n",
    "        articles = []\n",
    "\n",
    "        for id in trending_list[:top_k]:\n",
    "            post = requests.get(f\"{self.base_url}/item/{id}.json?print=pretty\").json()\n",
    "            if \"url\" in post:\n",
    "                try:\n",
    "                    content = self.fetch_content(post[\"url\"])\n",
    "                    articles.append(\n",
    "                        {\"content\": content, \"title\": post[\"title\"], \"url\": post[\"url\"]}\n",
    "                    )\n",
    "\n",
    "                except requests.RequestException as e:\n",
    "                    print(f\"Can't download {post}, skipped. Error: {e}\")\n",
    "                # except:\n",
    "                # \tprint(f\"Can't download {post}, skipped\")\n",
    "            elif \"text\" in post:\n",
    "                articles.append(\n",
    "                    {\n",
    "                        \"content\": post[\"text\"],\n",
    "                        \"title\": post[\"title\"],\n",
    "                        \"url\": f\"https://news.ycombinator.com/item?id={id}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return articles\n",
    "\n",
    "    def fetch_content(self, url: str) -> str:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "\n",
    "\n",
    "def summarize_articles(articles: List[dict]) -> str:\n",
    "    prompt_template = \"\"\"\n",
    "\tYou will be provided a few of the top posts in HackerNews, followed by their URL.\n",
    "\tFor each post, provide a brief summary followed by the URL the full post can be found at.\n",
    "\n",
    "\tPosts:\n",
    "\t{article_summaries}\n",
    "\t\"\"\"\n",
    "\n",
    "    # Prepare the article summaries\n",
    "    article_summaries = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Content: {article['content']}\\nURL: {article['url']}\"\n",
    "            for article in articles\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"article_summaries\"], template=prompt_template\n",
    "    )\n",
    "\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.1:8b\",\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    chain = prompt | llm | output_parser\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "            \"article_summaries\": article_summaries,\n",
    "        }\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a brief summary of each post followed by the URL where the full content can be found:\n",
      "\n",
      "**Post 1:**\n",
      "Summary: The author uses Anthropic Claude 3.5 to create calendar entries from an image of their jazz piano lesson schedule. They upload the image, ask Claude to identify the dates outlined in green, and then use those dates to create a calendar ICS file with appointments for each date at 2pm Pacific Time.\n",
      "URL: https://gregsramblings.com/stupid-but-useful-ai-tricks-creating-calendar-entries-from-an-image-using-anthropic-claude-35\n",
      "\n",
      "**Post 2:**\n",
      "Summary: The author asks the community what they're working on and if they have any new ideas. This is a general question with no specific content.\n",
      "URL: https://news.ycombinator.com/item?id=41342017\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "fetcher = HackernewsNewestFetcher()\n",
    "articles = fetcher.fetch_articles(top_k=2)\n",
    "summaries = summarize_articles(articles)\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
