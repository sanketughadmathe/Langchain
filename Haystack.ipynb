{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haystack Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document\n",
    "from haystack_integrations.components.embedders.ollama import OllamaDocumentEmbedder\n",
    "\n",
    "embedder = OllamaDocumentEmbedder(model=\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        content=\"Haystack is an open source AI framework to build full AI applications in Python\"\n",
    "    ),\n",
    "    Document(content=\"You can build AI Pipelines by combining Components\"),\n",
    "]\n",
    "\n",
    "result = embedder.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = embedder.run(documents)\n",
    "print(result[\"documents\"][0].embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a Document Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haystack import Pipeline\n",
    "# from haystack.components.converters import PyPDFToDocument, TextFileToDocument\n",
    "# from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "# from haystack.components.writers import DocumentWriter\n",
    "# from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "# from haystack.document_stores.types import DuplicatePolicy\n",
    "# from haystack_integrations.components.embedders.ollama import OllamaDocumentEmbedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing documents with embeddings into a document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack_integrations.components.embedders.ollama import OllamaDocumentEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
    "\n",
    "embedder = OllamaDocumentEmbedder(model=\"llama3.1:8b\")\n",
    "\n",
    "cleaner = DocumentCleaner()\n",
    "file_converter = TextFileToDocument()\n",
    "splitter = DocumentSplitter()\n",
    "writer = DocumentWriter(document_store=document_store, policy=DuplicatePolicy.OVERWRITE)\n",
    "\n",
    "indexing_pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipeline.add_component(\"converter\", file_converter)\n",
    "indexing_pipeline.add_component(\"splitter\", splitter)\n",
    "indexing_pipeline.add_component(\"cleaner\", cleaner)\n",
    "indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "indexing_pipeline.add_component(\"writer\", writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipeline.connect(\"converter\", \"cleaner\")\n",
    "indexing_pipeline.connect(\"cleaner\", \"splitter\")\n",
    "# indexing_pipeline.connect(\"converter\", \"splitter\")\n",
    "indexing_pipeline.connect(\"splitter\", \"embedder\")\n",
    "indexing_pipeline.connect(\"embedder\", \"writer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipeline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipeline.run({\"converter\": {\"sources\": [\"data/davinci.txt\"]}})\n",
    "document_store.filter_documents()[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a document search pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack_integrations.components.embedders.ollama import OllamaTextEmbedder\n",
    "\n",
    "query_embedder = OllamaTextEmbedder(model=\"llama3.1:8b\")\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
    "\n",
    "document_search = Pipeline()\n",
    "\n",
    "document_search.add_component(\"query_embedder\", query_embedder)\n",
    "document_search.add_component(\"retriever\", retriever)\n",
    "\n",
    "document_search.connect(\"query_embedder.embedding\", \"retriever.query_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_search.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "\n",
    "# from haystack_integrations.components.embedders.ollama import OllamaTextEmbedder\n",
    "# query_embedder = OllamaTextEmbedder(model=\"llama3.1:8b\")\n",
    "# retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
    "\n",
    "# document_search = Pipeline()\n",
    "\n",
    "# document_search.add_component(\"query_embedder\", query_embedder)\n",
    "# document_search.add_component(\"retriever\", retriever)\n",
    "\n",
    "# document_search.connect(\"query_embedder.embedding\", \"retriever.query_embedding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a search query\n",
    "query = \"What is the content of the Davinci document?\"\n",
    "result = document_search.run({\"query_embedder\": {\"text\": query}})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How old was Davinci when he died?\"\n",
    "\n",
    "results = document_search.run({\"query_embedder\": {\"text\": question}})\n",
    "\n",
    "for i, document in enumerate(results[\"retriever\"][\"documents\"]):\n",
    "    print(\"\\n--------------\\n\")\n",
    "    print(f\"DOCUMENT {i}\")\n",
    "    print(document.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How old was Davinci when he died?\"\n",
    "\n",
    "results = document_search.run(\n",
    "    {\"query_embedder\": {\"text\": question}, \"retriever\": {\"top_k\": 3}}\n",
    ")\n",
    "\n",
    "for i, document in enumerate(results[\"retriever\"][\"documents\"]):\n",
    "    print(\"\\n--------------\\n\")\n",
    "    print(f\"DOCUMENT {i}\")\n",
    "    print(document.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Where was Davinci born?\"\n",
    "\n",
    "results = document_search.run(\n",
    "    {\"query_embedder\": {\"text\": question}, \"retriever\": {\"top_k\": 3}}\n",
    ")\n",
    "\n",
    "for i, document in enumerate(results[\"retriever\"][\"documents\"]):\n",
    "    print(\"\\n--------------\\n\")\n",
    "    print(f\"DOCUMENT {i}\")\n",
    "    print(document.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When did Davinci live in Rome?\"\n",
    "\n",
    "results = document_search.run(\n",
    "    {\"query_embedder\": {\"text\": question}, \"retriever\": {\"top_k\": 3}}\n",
    ")\n",
    "\n",
    "for i, document in enumerate(results[\"retriever\"][\"documents\"]):\n",
    "    print(\"\\n--------------\\n\")\n",
    "    print(f\"DOCUMENT {i}\")\n",
    "    print(document.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haystack Build Customized RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haystack import Pipeline\n",
    "# from haystack.utils.auth import Secret\n",
    "# from haystack.components.builders import PromptBuilder\n",
    "# from haystack.components.converters import HTMLToDocument\n",
    "# from haystack.components.fetchers import LinkContentFetcher\n",
    "# from haystack.components.generators import OpenAIGenerator\n",
    "# from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "# from haystack.components.writers import DocumentWriter\n",
    "# from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "# from haystack_integrations.components.embedders.cohere.document_embedder import CohereDocumentEmbedder\n",
    "# from haystack_integrations.components.embedders.cohere.text_embedder import CohereTextEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.utils.auth import Secret\n",
    "from haystack_integrations.components.embedders.cohere.document_embedder import (\n",
    "    CohereDocumentEmbedder,\n",
    ")\n",
    "from haystack_integrations.components.embedders.cohere.text_embedder import (\n",
    "    CohereTextEmbedder,\n",
    ")\n",
    "# from haystack_integrations.components.embedders.cohere import CohereDocumentEmbedder, CohereTextEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "fetcher = LinkContentFetcher()\n",
    "converter = HTMLToDocument()\n",
    "embedder = CohereDocumentEmbedder(\n",
    "    model=\"embed-english-v3.0\", api_base_url=os.getenv(\"CO_API_URL\")\n",
    ")\n",
    "writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "indexing = Pipeline()\n",
    "indexing.add_component(\"fetcher\", fetcher)\n",
    "indexing.add_component(\"converter\", converter)\n",
    "indexing.add_component(\"embedder\", embedder)\n",
    "indexing.add_component(\"writer\", writer)\n",
    "\n",
    "indexing.connect(\"fetcher.streams\", \"converter.sources\")\n",
    "indexing.connect(\"converter\", \"embedder\")\n",
    "indexing.connect(\"embedder\", \"writer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI  # Assuming you're using OpenAI\n",
    "from langchain.text_loaders import TextLoader\n",
    "\n",
    "\n",
    "# Function to fetch Hacker News articles\n",
    "def fetch_hackernews_articles(top_k):\n",
    "    trending_list = requests.get(\n",
    "        url=\"https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty\"\n",
    "    ).json()[0:top_k]\n",
    "    articles = []\n",
    "    for id in trending_list:\n",
    "        post = requests.get(\n",
    "            url=f\"https://hacker-news.firebaseio.com/v0/item/{id}.json?print=pretty\"\n",
    "        ).json()\n",
    "        if \"url\" in post:\n",
    "            try:\n",
    "                articles.append(post[\"text\"])\n",
    "            except:\n",
    "                print(f\"Can't download {post}, skipped\")\n",
    "        elif \"text\" in post:\n",
    "            articles.append(post[\"text\"])\n",
    "        else:\n",
    "            print(f\"Can't download {post}, skipped\")\n",
    "    return articles\n",
    "\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"\n",
    "You will be provided a few of the top posts in HackerNews.\n",
    "For each post, provide a brief summary followed by the URL the full post can be found at.\n",
    "\n",
    "Posts:\n",
    "{% for article in articles %}\n",
    "  {{ article }}\n",
    "  URL: {{ article.meta.get('url', '') }}  # Handle posts without URL\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "# Create the LangChain pipeline\n",
    "llm = OpenAI()  # Replace with your preferred LLM\n",
    "articles_loader = TextLoader(texts=fetch_hackernews_articles(top_k=3))\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template, text_loader=articles_loader)\n",
    "\n",
    "# Run the pipeline\n",
    "summaries = chain.run()\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "class HackernewsNewestFetcher:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://hacker-news.firebaseio.com/v0\"\n",
    "\n",
    "    def fetch_articles(self, top_k: int) -> List[dict]:\n",
    "        response = requests.get(f\"{self.base_url}/topstories.json?print=pretty\")\n",
    "        trending_list = response.json()\n",
    "        articles = []\n",
    "\n",
    "        for id in trending_list[:top_k]:\n",
    "            post = requests.get(f\"{self.base_url}/item/{id}.json?print=pretty\").json()\n",
    "            if \"url\" in post:\n",
    "                try:\n",
    "                    content = self.fetch_content(post[\"url\"])\n",
    "                    articles.append(\n",
    "                        {\"content\": content, \"title\": post[\"title\"], \"url\": post[\"url\"]}\n",
    "                    )\n",
    "\n",
    "                except requests.RequestException as e:\n",
    "                    print(f\"Can't download {post}, skipped. Error: {e}\")\n",
    "                # except:\n",
    "                # \tprint(f\"Can't download {post}, skipped\")\n",
    "            elif \"text\" in post:\n",
    "                articles.append(\n",
    "                    {\n",
    "                        \"content\": post[\"text\"],\n",
    "                        \"title\": post[\"title\"],\n",
    "                        \"url\": f\"https://news.ycombinator.com/item?id={id}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return articles\n",
    "\n",
    "    def fetch_content(self, url: str) -> str:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "\n",
    "\n",
    "def summarize_articles(articles: List[dict]) -> str:\n",
    "    prompt_template = \"\"\"\n",
    "\tYou will be provided a few of the top posts in HackerNews, followed by their URL.\n",
    "\tFor each post, provide a brief summary followed by the URL the full post can be found at.\n",
    "\n",
    "\tPosts:\n",
    "\t{article_summaries}\n",
    "\t\"\"\"\n",
    "\n",
    "    # Prepare the article summaries\n",
    "    article_summaries = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Content: {article['content']}\\nURL: {article['url']}\"\n",
    "            for article in articles\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"article_summaries\"], template=prompt_template\n",
    "    )\n",
    "\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.1:8b\",\n",
    "        temperature=0,\n",
    "        # other params...\n",
    "    )\n",
    "\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    chain = prompt | llm | output_parser\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "            \"article_summaries\": article_summaries,\n",
    "        }\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "# Main execution\n",
    "fetcher = HackernewsNewestFetcher()\n",
    "articles = fetcher.fetch_articles(top_k=2)\n",
    "\n",
    "summaries = summarize_articles(articles)\n",
    "\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "fetcher = HackernewsNewestFetcher()\n",
    "articles = fetcher.fetch_articles(top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = summarize_articles(articles)\n",
    "\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_summaries = \"\\n\\n\".join(\n",
    "    [f\"Content: {article['content']}\\nURL: {article['url']}\" for article in articles]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You will be provided a few of the top posts in HackerNews, followed by their URL.\n",
    "For each post, provide a brief summary followed by the URL the full post can be found at.\n",
    "\n",
    "Posts:\n",
    "{article_summaries}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"article_summaries\"], template=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(article_summaries=article_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "output = chain.invoke(\n",
    "    {\n",
    "        \"article_summaries\": article_summaries,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "class HackernewsNewestFetcher:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://hacker-news.firebaseio.com/v0\"\n",
    "\n",
    "    def fetch_articles(self, top_k: int) -> List[dict]:\n",
    "        response = requests.get(f\"{self.base_url}/topstories.json?print=pretty\")\n",
    "        trending_list = response.json()\n",
    "        articles = []\n",
    "\n",
    "        for id in trending_list[:top_k]:\n",
    "            post = requests.get(f\"{self.base_url}/item/{id}.json?print=pretty\").json()\n",
    "            if \"url\" in post:\n",
    "                try:\n",
    "                    content = self.fetch_content(post[\"url\"])\n",
    "                    articles.append(\n",
    "                        {\"content\": content, \"title\": post[\"title\"], \"url\": post[\"url\"]}\n",
    "                    )\n",
    "\n",
    "                except requests.RequestException as e:\n",
    "                    print(f\"Can't download {post}, skipped. Error: {e}\")\n",
    "                # except:\n",
    "                # \tprint(f\"Can't download {post}, skipped\")\n",
    "            elif \"text\" in post:\n",
    "                articles.append(\n",
    "                    {\n",
    "                        \"content\": post[\"text\"],\n",
    "                        \"title\": post[\"title\"],\n",
    "                        \"url\": f\"https://news.ycombinator.com/item?id={id}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return articles\n",
    "\n",
    "    def fetch_content(self, url: str) -> str:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "\n",
    "\n",
    "def summarize_articles(articles: List[dict]) -> str:\n",
    "    prompt_template = \"\"\"\n",
    "\tYou will be provided a few of the top posts in HackerNews, followed by their URL.\n",
    "\tFor each post, provide a brief summary followed by the URL the full post can be found at.\n",
    "\n",
    "\tPosts:\n",
    "\t{article_summaries}\n",
    "\t\"\"\"\n",
    "\n",
    "    # Prepare the article summaries\n",
    "    article_summaries = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Content: {article['content']}\\nURL: {article['url']}\"\n",
    "            for article in articles\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"article_summaries\"], template=prompt_template\n",
    "    )\n",
    "\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.1:8b\",\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    chain = prompt | llm | output_parser\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "            \"article_summaries\": article_summaries,\n",
    "        }\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a brief summary of the article:\n",
      "\n",
      "**Summary:** This is an open-ended question posted to Hacker News, asking users what projects or ideas they are currently working on.\n",
      "\n",
      "**URL:** https://news.ycombinator.com/item?id=41342017\n",
      "\n",
      "The text discusses the importance of thinking in graphs rather than lists when it comes to network security. It highlights how attackers can use visualizations and connections between systems to compromise a High Value Asset (HVA) by targeting dependent elements, such as terminal servers, admin accounts, and certificate authorities.\n",
      "\n",
      "Key points:\n",
      "\n",
      "* Attackers think in graphs, not lists, which gives them an advantage over defenders who rely on mental models or outdated diagrams.\n",
      "* Compromising a terminal server can lead to compromising admin accounts on other machines, which can then be used to access the HVA.\n",
      "* Local admin accounts with common passwords, file servers, print servers, and certificate authorities are all potential security dependencies that can create unwanted edges in the graph.\n",
      "* Defenders should visualize their network as a graph and implement controls to prune it, such as infrastructure partitioning, credential silos, privilege minimization, and two-factor authentication.\n",
      "\n",
      "The text also mentions several papers on attack graphs and provides further reading suggestions for defenders who want to learn more about this topic.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import StrOutputParser\n",
    "from typing import List\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "class HackernewsNewestFetcher:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://hacker-news.firebaseio.com/v0\"\n",
    "\n",
    "    def fetch_articles(self, top_k: int) -> List[dict]:\n",
    "        response = requests.get(f\"{self.base_url}/topstories.json?print=pretty\")\n",
    "        trending_list = response.json()\n",
    "        articles = []\n",
    "\n",
    "        for id in trending_list[:top_k]:\n",
    "            post = requests.get(f\"{self.base_url}/item/{id}.json?print=pretty\").json()\n",
    "            if \"url\" in post:\n",
    "                try:\n",
    "                    content = self.fetch_content(post[\"url\"])\n",
    "                    articles.append(\n",
    "                        {\"content\": content, \"title\": post[\"title\"], \"url\": post[\"url\"]}\n",
    "                    )\n",
    "\n",
    "                except requests.RequestException as e:\n",
    "                    print(f\"Can't download {post}, skipped. Error: {e}\")\n",
    "                # except:\n",
    "                # \tprint(f\"Can't download {post}, skipped\")\n",
    "            elif \"text\" in post:\n",
    "                articles.append(\n",
    "                    {\n",
    "                        \"content\": post[\"text\"],\n",
    "                        \"title\": post[\"title\"],\n",
    "                        \"url\": f\"https://news.ycombinator.com/item?id={id}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return articles\n",
    "\n",
    "    def fetch_content(self, url: str) -> str:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "\n",
    "\n",
    "def summarize_articles(articles: List[dict]) -> str:\n",
    "    prompt_template = \"\"\"\n",
    "    Summarize the following article from HackerNews. Provide a brief summary followed by the URL the full post can be found at.\n",
    "\n",
    "    Article:\n",
    "    Title: {title}\n",
    "    Content: {content}\n",
    "    URL: {url}\n",
    "\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"title\", \"content\", \"url\"], template=prompt_template\n",
    "    )\n",
    "\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.1:8b\",\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    chain = (prompt | llm | StrOutputParser()).invoke\n",
    "\n",
    "    summaries = []\n",
    "    for article in articles:\n",
    "        summary = chain(\n",
    "            {\n",
    "                \"title\": article[\"title\"],\n",
    "                \"content\": article[\"content\"],\n",
    "                \"url\": article[\"url\"],\n",
    "            }\n",
    "        )\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return \"\\n\\n\".join(summaries)\n",
    "\n",
    "\n",
    "# Main execution\n",
    "fetcher = HackernewsNewestFetcher()\n",
    "articles = fetcher.fetch_articles(top_k=2)\n",
    "summaries = summarize_articles(articles)\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a brief summary of the article:\n",
      "\n",
      "The article \"Defenders think in lists, attackers think in graphs (2015)\" suggests that defenders and attackers have different mindsets when it comes to security. Defenders tend to think in linear, list-based ways, while attackers think in non-linear, graph-based ways. This difference in thinking makes it easier for attackers to outmaneuver defenders.\n",
      "\n",
      "URL: https://github.com/JohnLaTwC/Shared/blob/master/Defenders%20think%20in%20lists.%20Attackers%20think%20in%20graphs.%20As%20long%20as%20this%20is%20true%2C%20attackers%20win.md\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import StrOutputParser\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class HackerNewsFetcher:\n",
    "    \"\"\"A class to fetch articles from Hacker News.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://hacker-news.firebaseio.com/v0\"\n",
    "\n",
    "    def _fetch_article(self, article_id: int) -> dict:\n",
    "        \"\"\"Fetch an article from the Hacker News API.\n",
    "\n",
    "        Args:\n",
    "            article_id (int): The ID of the article to fetch.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the article's data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            post = requests.get(f\"{self.base_url}/item/{article_id}.json\").json()\n",
    "            return post\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to fetch article {article_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _fetch_content(self, url: str) -> str:\n",
    "        \"\"\"Fetch the content of a web page.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL of the web page.\n",
    "\n",
    "        Returns:\n",
    "            str: The content of the web page as text.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            return soup.get_text()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to fetch content from {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def fetch_articles(self, top_k: int) -> List[dict]:\n",
    "        \"\"\"Fetch the top K articles from Hacker News.\n",
    "\n",
    "        Args:\n",
    "            top_k (int): The number of articles to fetch.\n",
    "\n",
    "        Returns:\n",
    "            List[dict]: A list of dictionaries containing the article's data.\n",
    "        \"\"\"\n",
    "        response = requests.get(f\"{self.base_url}/topstories.json\")\n",
    "        trending_list = response.json()\n",
    "\n",
    "        articles = []\n",
    "        for id in trending_list[:top_k]:\n",
    "            post = self._fetch_article(id)\n",
    "            if post and \"url\" in post:\n",
    "                content = self._fetch_content(post[\"url\"])\n",
    "                article_data = {\n",
    "                    \"content\": content,\n",
    "                    \"title\": post[\"title\"],\n",
    "                    \"url\": post[\"url\"],\n",
    "                }\n",
    "                articles.append(article_data)\n",
    "\n",
    "        return articles\n",
    "\n",
    "\n",
    "class ArticleSummarizer:\n",
    "    \"\"\"A class to summarize articles fetched from Hacker News.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.prompt_template = \"\"\"\n",
    "            Summarize the following article from HackerNews. Provide a brief summary followed by the URL the full post can be found at.\n",
    "\n",
    "            Article:\n",
    "            Title: {title}\n",
    "            Content: {content}\n",
    "            URL: {url}\n",
    "\n",
    "            Summary:\n",
    "            \"\"\"\n",
    "\n",
    "    def summarize_articles(self, articles: List[dict]) -> str:\n",
    "        \"\"\"Summarize a list of articles fetched from Hacker News.\n",
    "\n",
    "        Args:\n",
    "            articles (List[dict]): A list of dictionaries containing the article's data.\n",
    "\n",
    "        Returns:\n",
    "            str: A string summarizing each article.\n",
    "        \"\"\"\n",
    "        llm = ChatOllama(model=\"llama3.1:8b\", temperature=0)\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"title\", \"content\", \"url\"], template=self.prompt_template\n",
    "        )\n",
    "\n",
    "        chain = (prompt_template | llm | StrOutputParser()).invoke\n",
    "\n",
    "        summaries = []\n",
    "        for article in articles:\n",
    "            summary = chain(\n",
    "                {\n",
    "                    \"title\": article[\"title\"],\n",
    "                    \"content\": article[\"content\"],\n",
    "                    \"url\": article[\"url\"],\n",
    "                }\n",
    "            )\n",
    "            summaries.append(summary)\n",
    "\n",
    "        return \"\\n\\n\".join(summaries)\n",
    "\n",
    "\n",
    "# Main execution\n",
    "fetcher = HackerNewsFetcher()\n",
    "articles = fetcher.fetch_articles(top_k=2)\n",
    "summarizer = ArticleSummarizer()\n",
    "summaries = summarizer.summarize_articles(articles)\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "fetcher = HackernewsNewestFetcher()\n",
    "articles = fetcher.fetch_articles(top_k=2)\n",
    "summaries = summarize_articles(articles)\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_articles(articles: List[dict]) -> str:\n",
    "    prompt_template = \"\"\"\n",
    "\tSummarize the following article from HackerNews. Provide a brief summary followed by the URL the full post can be found at.\n",
    "\n",
    "\tArticle:\n",
    "\tTitle: {title}\n",
    "\tContent: {content}\n",
    "\tURL: {url}\n",
    "\n",
    "\tSummary:\n",
    "\t\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"title\", \"content\", \"url\"], template=prompt_template\n",
    "    )\n",
    "\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.1:8b\",\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    # output_parser = StrOutputParser()\n",
    "\n",
    "    # chain = prompt | llm | output_parser\n",
    "\n",
    "    # output = chain.invoke(\n",
    "    # \t{\n",
    "    # \t\t\"article_summaries\":article_summaries,\n",
    "    # \t}\n",
    "    # )\n",
    "\n",
    "    chain = (prompt | llm | StrOutputParser()).invoke\n",
    "\n",
    "    summaries = []\n",
    "    for article in articles:\n",
    "        summary = chain(\n",
    "            {\n",
    "                \"title\": article[\"title\"],\n",
    "                \"content\": article[\"content\"],\n",
    "                \"url\": article[\"url\"],\n",
    "            }\n",
    "        )\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return \"\\n\\n\".join(summaries)\n",
    "\n",
    "\n",
    "# Main execution\n",
    "fetcher = HackernewsNewestFetcher()\n",
    "articles = fetcher.fetch_articles(top_k=2)\n",
    "summaries = summarize_articles(articles)\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.schema import AgentAction, AgentFinish, Document\n",
    "\n",
    "# Initialize Ollama embeddings and ChatOllama\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.1:8b\")\n",
    "# llm = ChatOllama(model=\"llama2\")\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Create documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Retrievers: Retrieves relevant documents to a user query using keyword search or semantic search.\"\n",
    "    ),\n",
    "    Document(page_content=\"Embedders: Creates embeddings for text or documents.\"),\n",
    "    Document(\n",
    "        page_content=\"Generators: Use a number of model providers to generate answers or content based on a prompt\"\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"File Converters: Converts different file types like TXT, Markdown, PDF, etc. into a Haystack Document type\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create vector store\n",
    "faiss_db = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Create RAG prompt template\n",
    "rag_prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "\t\tAnswer the following query given the documents.\n",
    "\t\tIf the answer is not contained within the documents, reply with 'no_answer'\n",
    "\t\tQuery: {query}\n",
    "\t\tDocuments:\n",
    "\t\t{context}\n",
    "\t\"\"\",\n",
    "    input_variables=[\"query\", \"context\"],\n",
    ")\n",
    "\n",
    "# Create RAG chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=faiss_db.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt_template},\n",
    ")\n",
    "\n",
    "# Create websearch prompt template\n",
    "websearch_prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "\t\tAnswer the following query given the documents retrieved from the web.\n",
    "\t\tYour answer should indicate that your answer was generated from websearch.\n",
    "\t\tYou can also reference the URLs that the answer was generated from\n",
    "\t\tQuery: {query}\n",
    "\t\tDocuments:\n",
    "\t\t{context}\n",
    "\t\"\"\",\n",
    "    input_variables=[\"query\", \"context\"],\n",
    ")\n",
    "\n",
    "# Create websearch tool\n",
    "search = DuckDuckGoSearchRun()\n",
    "websearch_tool = Tool(\n",
    "    name=\"Web Search\",\n",
    "    func=search.run,\n",
    "    description=\"Useful for when you need to answer questions about current events or the current state of the world\",\n",
    ")\n",
    "\n",
    "\n",
    "# Create custom output parser\n",
    "class CustomOutputParser:\n",
    "    def parse(self, text: str) -> AgentAction | AgentFinish:\n",
    "        if \"no_answer\" in text.lower():\n",
    "            return AgentAction(tool=\"Web Search\", tool_input=text, log=text)\n",
    "        return AgentFinish(return_values={\"output\": text}, log=text)\n",
    "\n",
    "\n",
    "# Create agent prompt\n",
    "agent_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \tAnswer the following question:\n",
    "\t\t{query}\n",
    "\t\tIf you don't know the answer, respond with 'no_answer'.\n",
    "\t\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    ")\n",
    "\n",
    "# Create LLM chain for the agent\n",
    "llm_chain = rag_chain\n",
    "\n",
    "# Create the agent\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=CustomOutputParser(),\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=[\"Web Search\"],\n",
    ")\n",
    "\n",
    "# Create tools\n",
    "tools = [websearch_tool]\n",
    "\n",
    "# Create the agent executor\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory(memory_key=\"chat_history\"),\n",
    ")\n",
    "\n",
    "# Test the pipeline\n",
    "print(\"Query: What is a retriever for?\")\n",
    "result = agent_executor.run(\"What is a retriever for?\")\n",
    "print(f\"Result: {result}\\n\")\n",
    "\n",
    "print(\"Query: What Mistral components does Langchain have?\")\n",
    "result = agent_executor.run(\"What Mistral components does Langchain have?\")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
